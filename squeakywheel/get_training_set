#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep 18 10:14:39 2018

@author: rcarns
"""

def RetrieveTweets(searchQuery,dataframe,maxTweets=10000):
    # the below code draws from 
    # https://stackoverflow.com/questions/38555191/get-all-twitter-mentions-using-tweepy-for-users-with-millions-of-followers

    import tweepy
    import pandas as pd
    import numpy as np
    import pickle
    from connections import twitterapi
    
    tweetsPerQry = 100
    sinceId = None

    

    max_id = -1

    tweetCount = 0
    tweetProgress = 0
    #fName = 'tweetlist.txt'
    #with open(fName, 'w') as fname:
    list_of_tweets = []
    while tweetCount < maxTweets:
            try:
                if (max_id <= 0):
                    if (not sinceId):
                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry, tweet_mode='extended')
                    else:
                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                                since_id=sinceId)
                else:
                    if (not sinceId):
                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                                max_id=str(max_id - 1),tweet_mode='extended')
                    else:
                        new_tweets = api.search(q=searchQuery, count=tweetsPerQry,
                                                max_id=str(max_id - 1),
                                                since_id=sinceId)
                if not new_tweets:
                    print("No more tweets found")
                    break
                for tweet in new_tweets:
                    #print(tweet.created_at.strftime('%x %X')+' '+tweet.full_text)
                    list_of_tweets.append(tweet._json)
                tweetCount += len(new_tweets)
                if tweetCount//1000>tweetProgress:
                    print("Downloaded {0} tweets".format(tweetCount))
                    tweetProgress+=1
                max_id = new_tweets[-1].id
            except tweepy.TweepError as e:
                # Just exit if any error
                print("some error : " + str(e))
                break

    picklefile = open(storagefile,'wb')
    pickle.dump(list_of_tweets,picklefile)
    picklefile.close()
    return tweetCount
    
import tweepy
import pandas as pd
import numpy as np
import pickle
from connections import twitterapi, postgresconnect

# check how many searches are left in this time window
check = api.rate_limit_status()
reset_time = check['resources']['search']['/search/tweets']['reset']
import datetime
reset_formatted = datetime.datetime.fromtimestamp(reset_time).strftime('%X')
print(reset_formatted)
print(check['resources']['search'])

# get names of corporate twitters
CorpTwitters = pd.read_csv('CorpTwittersAll.txt',names=['Main','Support','Sector'])
#CorpTwitters = CorpTwitters[CorpTwitters['Sector']=='retail']
ComplaintAccounts = CorpTwitters['Support'].apply(lambda x: x[1:].lower())
MainAccounts = CorpTwitters['Main'].apply(lambda x: x[1:].lower())
maxtweets = 10000
for account in ComplaintAccounts:'
    type = 'complaint'
    atuser = '@'+account
    import time
    retweet_filter='-filter:retweets'
    reply_filter = '-filter:replies'
    searchQuery = atuser+' AND '+retweet_filter+' AND '+reply_filter# + 'AND until:2018-09-11'
    storagefile = type+'tweets'+time.strftime("%Y%m%d-%H%M%S")+'.dat'
    tweetCount = RetrieveTweets(searchQuery,storagefile,maxtweets)
    maxtweets = tweetCount
    import shutil
    shutil.copy(storagefile,type+'tweets.dat')

for account in ComplaintAccounts:
    type = 'neutral'
    atuser = '@'+account
    import time
    retweet_filter='-filter:retweets'
    reply_filter = '-filter:replies'
    searchQuery = atuser+' AND '+retweet_filter+' AND '+reply_filter# + 'AND until:2018-09-11'
    storagefile = type+'tweets'+time.strftime("%Y%m%d-%H%M%S")+'.dat'
    tweetCount = RetrieveTweets(searchQuery,storagefile,maxtweets)
    #maxtweets = tweetCount
    import shutil
    shutil.copy(storagefile,type+'tweets.dat')
    
#for type in ['complaint','neutral']:
#    if type=='complaint':
#        atuser = '@'+' OR @'.join(ComplaintAccounts)
#        print(atuser)
#    elif type=='neutral':
#        atuser = '@'+' OR @'.join(MainAccounts)
#        print(atuser)
#
#    import time
#    retweet_filter='-filter:retweets'
#    reply_filter = '-filter:replies'
#    searchQuery = atuser+' AND '+retweet_filter+' AND '+reply_filter# + 'AND until:2018-09-11'
#    storagefile = type+'tweets'+time.strftime("%Y%m%d-%H%M%S")+'.dat'
#    tweetCount = RetrieveTweets(searchQuery,storagefile,maxtweets)
#    maxtweets = tweetCount
#    import shutil
#    shutil.copy(storagefile,type+'tweets.dat')



                