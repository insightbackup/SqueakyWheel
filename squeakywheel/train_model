#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Sep 20 10:48:48 2018

@author: rcarns
"""

import keras
import nltk
import pandas as pd
import numpy as np
import re
import codecs
import pickle

train_tweets = pd.read_csv('train_tweets.csv')

# Import stopwords with scikit-learn
#from sklearn.feature_extraction import text
#stop = text.ENGLISH_STOP_WORDS
#train_tweets['text'] = train_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')
train_tweets.loc[:,"tokens"] = train_tweets.loc[:,"text"].apply(tokenizer.tokenize)

#clean_questions = pd.read_csv('clean_questions.csv')
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

def tv(data):
    vectorizer = TfidfVectorizer()
    emb = vectorizer.fit_transform(data)
    return emb, vectorizer

def cv(data):
    count_vectorizer = CountVectorizer()
    emb = count_vectorizer.fit_transform(data)
    return emb, count_vectorizer


train_tweets = train_tweets.dropna('rows','any')

list_corpus = train_tweets["text"].tolist()
list_labels = train_tweets["class_label"].tolist()

tf_corpus,tfidf_vectorizer = tv(list_corpus)

X_train_counts, X_test_counts, y_train, y_test = train_test_split(tf_corpus, list_labels, test_size=0.2, random_state=40)



from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(C=1.0, penalty='l2',class_weight='balanced', solver='newton-cg', 
                         multi_class='multinomial', n_jobs=-1, random_state=40)
clf.fit(X_train_counts, y_train)

modelpickle = open('model.pkl','wb')
pickle.dump([clf,tfidf_vectorizer],modelpickle)

y_predicted_counts = clf.predict(X_test_counts)